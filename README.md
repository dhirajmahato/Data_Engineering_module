# Data_Engineering_module
Data Pipeline , ETL and other implementations

# Data Pipeline
- describe a workflow consisting of one or more tasks that ingest, move, and transform raw data from one or more sources to a destination.
- generally separate data pipelines into 2 categories: batch processing (most common) and real-time processing pipelines.
- 4 components
  1. Data source   ----> data ingestion
  2. Business logic (cleaning, filtering, applying logic)  --> Extract, Transform
  3. Destination (data warehouse, data lake) --> Load
  4. Schedular/Orchestration (for batch)
- Usecase
  - If a business sold software as a service, a Data Engineer might create a data pipeline that runs on a daily basis which takes some of the data generated by the software application, combine it with some data the marketing department has and send the data to a dashboard. The dashboard could then be used to better understand how customers are using the software.
  - Change Data Capture (CDC) Pipeline
  - ELT Pipeline
    - transform and load into destination - this lose the flexibility to re-compute raw data
    - transform the raw data in the destination itself.



## Key Components:
- **Databases**:
  - **Data Schema**:
- **Data Warehouse**: A conventional data warehousing solution typically involves copying data from transactional data stores into a relational database with a schema that's optimized for querying and building multidimensional models.
- 2. Data modeling - 
mapping out an information system, entity-relationship diagram


## Useful Links
- https://dataengineering.wiki/Concepts/Data+Pipeline
- https://datamike.hashnode.dev/data-platform-architecture-types
- https://webcache.googleusercontent.com/search?q=cache:https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3
- https://mydataschool.com/


