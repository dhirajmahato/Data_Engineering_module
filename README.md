# Data_Engineering_module
Data Pipeline , ETL and other implementations

# Data Pipeline
- describe a workflow consisting of one or more tasks that ingest, move, and transform raw data from one or more sources to a destination.
- generally separate data pipelines into 2 categories: batch processing (most common) and real-time processing pipelines.
- 4 components
  1. data source   ----> data ingestion
  2. business logic (cleaning, filtering, applying logic)
  3. Destination (date warehouse, data lake)



### Concepts:
- **Data Warehouse**: A conventional data warehousing solution typically involves copying data from transactional data stores into a relational database with a schema that's optimized for querying and building multidimensional models.
- 
# Key Components
## 1. Databases


## 2. Data modeling - 
mapping out an information system, entity-relationship diagram


## Useful Links
- https://dataengineering.wiki/Concepts/Data+Pipeline
- https://datamike.hashnode.dev/data-platform-architecture-types
- https://webcache.googleusercontent.com/search?q=cache:https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3
- https://mydataschool.com/


