# Data_Engineering_module
Data Pipeline , ETL and other implementations

# Data Pipeline
- describe a workflow consisting of one or more tasks that ingest, move, and transform raw data from one or more sources to a destination.
- generally separate data pipelines into 2 categories: batch processing (most common) and real-time processing pipelines.
- 4 components
  1. Data source   ----> data ingestion
  2. Business logic (cleaning, filtering, applying logic)  --> Extract, Transform
  3. Destination (data warehouse, data lake) --> Load
  4. Schedular/Orchestration (for batch)
- Usecase
  - If a business sold software as a service, a Data Engineer might create a data pipeline that runs on a daily basis which takes some of the data generated by the software application, combine it with some data the marketing department has and send the data to a dashboard. The dashboard could then be used to better understand how customers are using the software.
  - Change Data Capture (CDC) Pipeline - for transactional data
  - ELT Pipeline
    - transform and load into destination - this lose the flexibility to re-compute raw data
    - transform the raw data in the destination itself.



## Key Components:
- **[Databases](https://youtu.be/JZfeeyP-tCM)**:
  - **Desired Properties** - ACID
    1. A - Atomicity
    2. C - Consistency
    3. I - Isolation
    4. D - Durability
  - **Data modeling**: mapping out an information system how multiple parts are connected, entity-relationship diagram, 2 main categories:
    1. **Relational modeling** - using tables, columns, and rows to represent data.  A key component for relational modeling is normalization (reduces data redundancy).  **Great for *transactional/operational* workloads where data is constantly inserted, updated, or deleted. *Analytical* queries become slow at larger data scales.**
        - Relational Databases:  Each table can have one or more columns with unique identifiers (primary key) that point to an id column in another table (foreign key) which forms the relationship between the two tables.
            - Easier to do complex queries
            - Supports atomic transactions
            - Harder to scale (vertical scaling
        - Use cases: Customer Relationship Management, Enterprise resource planning, SaaA application, eCommerce and Web, Payment and booking system
    2. **Non-relational**
  
  **Database Schema**:
- **Data Warehouse**: A conventional data warehousing solution typically involves copying data from transactional data stores into a relational database with a schema that's optimized for querying and building multidimensional models.
- 2. 


## Useful Links
- https://dataengineering.wiki/Concepts/Data+Pipeline
- https://datamike.hashnode.dev/data-platform-architecture-types
- https://webcache.googleusercontent.com/search?q=cache:https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3
- https://mydataschool.com/


